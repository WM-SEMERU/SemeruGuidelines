# The Neophyte's Guide to Deep Learning 4 Software Engineering
> By @computoloco
>

# Curated List of Influential/Relevant studies in DL4SE
## Code Completion
- **Syntax-Aware On-the-Fly Code Completion** (2022), arxiv, Takerngsaksiri, W., et al. [[pdf]](https://arxiv.org/pdf/2211.04673)
- **CodeFill: Multi-token Code Completion by Jointly Learning from Structure and Naming Sequences** (2022), ICSE'22, Izadi, Maliheh, et al. [[pdf]](https://arxiv.org/pdf/2202.06689.pdf) [[code]](https://github.com/saltudelft/codefill)
- **Code Prediction by Feeding Trees to Transformers** (2021), ICSE'21, Kim, Seohyun, et al. [[pdf]](https://arxiv.org/pdf/2003.13848)
- **Fast and Memory-Efficient Neural Code Completion** (2020), arxiv 2020, Svyatkovskoy, Alexey, et al. [[pdf]](https://arxiv.org/pdf/2004.13651)

## Code Generation
- **MultiPL-E: A Scalable and Polyglot Approach to Benchmarking Neural Code Generation** (2023), TSE, Paul, Rishov, et al.  
- **CodeScore: Evaluating Code Generation by Learning Code Execution** (2023), arxiv, Dong, Yihong, et al. [[pdf]](https://arxiv.org/pdf/2301.09043)
- **Execution-based Code Generation using Deep Reinforcement Learning** (2023), arxiv, Shojaee, Parshin, et al. [[pdf]](https://arxiv.org/pdf/2301.13816)
- **SantaCoder: don't reach for the stars!** (2023), arxiv, Allal, Loubna Ben, et al. [[pdf]](https://arxiv.org/pdf/2301.03988.pdf)
- **Exploring and Evaluating Personalized Models for Code Generation**, FSE'22, Zlotchevski, Andrei, et al. [[pdf](https://arxiv.org/pdf/2208.13928.pdf)]
- **Natural Language to Code Generation in Interactive Data Science Notebooks** (2022), arxiv, Yin, Pengcheng, et al. [[pdf]](https://arxiv.org/pdf/2212.09248)
- **Execution-based Evaluation for Data Science Code Generation Models** (2022), arxiv, Huang, Junjie, et al. [[pdf]](https://arxiv.org/pdf/2211.09374)
- **Multi-lingual Evaluation of Code Generation Models** (2022), arxiv, Athiwaratkun, Ben, et al. [[pdf]](https://arxiv.org/pdf/2210.14868)[[code]](https://github.com/amazon-science/mbxp-exec-eval)
- **DocPrompting: Generating Code by Retrieving and Reading Docs** (2022), arxiv, Zhou, Shuyan, et al. [[pdf]](https://arxiv.org/pdf/2207.05987)
- **Language Models Can Teach Themselves to Program Better** (2022), arxiv, Haluptzok, Patrick, et al. [[pdf]](https://arxiv.org/pdf/2207.14502)
- **CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning** (2022), arxiv, Le, Hung, et al. [[pdf]](https://arxiv.org/pdf/2207.01780)
- **CERT: Continual Pre-Training on Sketches for Library-Oriented Code Generation** (2022), arxiv, Zan, Daoguang, et al. [[pdf]](https://arxiv.org/pdf/2206.06888)
- **NatGen: Generative pre-training by “Naturalizing” source code** (2022), FSE'22, Chakraborty, Saikat, et al. [[pdf]](https://arxiv.org/pdf/2206.07585)
- **StructCoder: Structure-Aware Transformer for Code Generation** (2022), arxiv, Tipirneni, Sindhu, et al. [[pdf]](https://arxiv.org/pdf/2206.05239)
- **Compilable Neural Code Generation with Compiler Feedback** (2022), arxiv 2022, Wang, Xin, et al. [[pdf]](https://arxiv.org/pdf/2203.05132.pdf)
- **Predictive Synthesis of API-Centric Code** (2022), arxiv 2022, Nam, Daye, et al. [[pdf]](https://arxiv.org/pdf/2201.03758.pdf)
- **Code Prediction by Feeding Trees to Transformers** (2020), arxiv 2020, Kim, Seohyun, et al. [[pdf]](https://arxiv.org/pdf/2003.13848)

## Code Summarization
- **Function Call Graph Context Encoding for Neural Source Code Summarization** (2023), TSE, Bansal, Aakash, et al. [[pdf](https://ieeexplore.ieee.org/abstract/document/10132550)]
- **CoSS: Leveraging Statement Semantics for Code Summarization** (2023), TSE, Shi, Chaochen, et al. [[pdf](https://ieeexplore.ieee.org/document/10068264)]
- **Towards Retrieval-Based Neural Code Summarization: A Meta-Learning Approach** (2023), TSE, Zhou, Ziyi, et al. [[pdf](https://ieeexplore.ieee.org/document/10021882)]
- **CLG-Trans: Contrastive Learning for Code Summarization via Graph Attention-based Transformer** (2023), SCP journal, Zeng, Jianwei, et al.
- **ClassSum: a deep learning model for class-level code summarization** (2022), Springer NCA, Li, Mingchen, et al. [[code]](https://github.com/classsum/ClassSum)
- **Boosting Code Summarization by Embedding Code Structures** (2022), COLING'22, Son, Jikyoeng, et al. [[pdf]](https://aclanthology.org/2022.coling-1.521.pdf)
- **Low-Resources Project-Specific Code Summarization** (2022), ASE'22, Xie, Rui, et al. [[pdf]](https://arxiv.org/pdf/2210.11843)
- **Few-shot training LLMs for project-specific code-summarization** (2022), arxiv, A., Toufique, and P. Devanbu. [[pdf]](https://arxiv.org/pdf/2207.04237)
- **Are We Building on the Rock? On the Importance of Data Preprocessing for Code Summarization** (2022), FSE'22, Shi, Lin, et al. [[pdf]](https://arxiv.org/pdf/2207.05579)
- **Modeling Hierarchical Syntax Structure with Triplet Position for Source Code Summarization** (2022), ACL'22, Guo, Juncai, et al. [[pdf]](https://aclanthology.org/2022.acl-long.37.pdf)
- **AST-Trans: Code Summarization with Efficient Tree-Structured Attention** (2022), ICSE'22, Tang, Ze, et al. [[pdf]](http://lichuanyi.info/files/papers/2022-Ze%20Tang-AST-Trans%20ICSE2022.pdf)
- **A Transformer-based Approach for Source Code Summarization** (2020), arxiv 2020, Ahmad, Wasi Uddin, et al. [[pdf]](https://arxiv.org/pdf/2005.00653)  

## Security
- **Systematically Finding Security Vulnerabilities in Black-Box Code Generation Models.** (2023), arxiv, Hajipour, Hossein, et al. [[pdf]](https://arxiv.org/pdf/2302.04012)
- **Controlling Large Language Models to Generate Secure and Vulnerable Code** (2023), arxiv, He, J., and M. Vechev [[pdf]](https://arxiv.org/pdf/2302.05319)
  
 ## Prompt Engineering
- **Repository-Level Prompt Generation for Large Language Models of Code** (2022), arxiv, Shrivastava, Disha, et al. [[pdf]](https://arxiv.org/pdf/2206.12839)

## Interpretability/Explainability/Probing
- **Demystifying What Code Summarization Models Learned** (2023), arxiv, Wang, Yu, and Ke Wang. [[pdf]](https://arxiv.org/pdf/2303.02333)
- **Interpretation-based Code Summarization** (2023), arxiv, Geng, Mingyang, et al. [[pdf]](https://www.researchgate.net/profile/Shangwen-Wang/publication/368755660_Interpretation-based_Code_Summarization/links/63f842890d98a97717b27fb8/Interpretation-based-Code-Summarization.pdf)

# DL4SE Basics
- A Systematic Literature Review on the Use of Deep Learning in Software Engineering Research [[web]](https://ml4code.github.io/publications/watson2021systematic/)
![checklist](https://user-images.githubusercontent.com/8354015/169404571-fb7a8d05-f501-4ebe-a02e-68617cd90281.png)


## DL Basics
- Deep Learning for Coders [[fast.AI]](https://www.fast.ai/)
- Natural Language Tutorial for DL Researchers [[tutorial]](https://github.com/graykode/nlp-tutorial)
- The Super Duper NLP Repo [[repo]](https://notebooks.quantumstat.com/)
- NBDEV: Developerment Environment [[tutorial]](https://nbdev.fast.ai/)

## DL Approaches
### Convolutional Nets
1. CNNs from different viewpoints [[web]](https://medium.com/impactai/cnns-from-different-viewpoints-fab7f52d159c)
2. How convolutional neural networks work, in depth [[web]](https://www.youtube.com/watch?v=JB8T_zN7ZC0)
3. Deep Learning Tips and Tricks cheatsheet [[web]](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-deep-learning-tips-and-tricks)
4. Convolutional Neural Networks cheatsheet [[web]](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks)
5. An Interactive Node-Link Visualization of Convolutional Neural Networks [[demo]](http://www.cs.cmu.edu/~aharley/vis/)
6. ConvNetJS MNIST [[demo]](https://cs.stanford.edu/people/karpathy/convnetjs/demo/mnist.html)
7. A guide to convolution arithmetic for deep learning [[paper]](https://arxiv.org/pdf/1603.07285.pdf)
8. Understanding Deep Residual Networks [[web]](https://shuzhanfan.github.io/2018/11/ResNet/)

### Stochastic Nets
1. Introduction to Restricted Boltzmann Machines [[web]](http://blog.echen.me/2011/07/18/introduction-to-restricted-boltzmann-machines/)
2. Monte Carlo Methods and Importance Sampling [[Lecture]](http://ib.berkeley.edu/labs/slatkin/eriq/classes/guest_lect/mc_lecture_notes.pdf)
3. Expectation Maximization (EM) [[blog]](https://karinknudson.com/expectationmaximization.html)

### Recurrent Nets
1. Illustrated Guide to Recurrent Neural Networks: Understanding the Intuition [[YT]](https://www.youtube.com/watch?v=LHXXI4-IEns)
2. Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM) [[YT]](https://www.youtube.com/watch?v=WCUNPb-5EYI)
3. Recurrent Neural Networks cheatsheet [[web]](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks)
4. The Illustrated Word2vec [[blog]](https://jalammar.github.io/illustrated-word2vec/)
5. Embedding Projector [[demo]](http://projector.tensorflow.org/)
6. Understanding LSTM Networks [[blog]](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
7. Illustrated Guide to LSTM's and GRU's: A step by step explanation [[YT]](https://www.youtube.com/watch?v=8HyCNIVRbSU)

### Transformers
1. Practicum: Attention and the Transformer [[YT]](https://www.youtube.com/watch?v=f01J0Dri-6k)
2. Understanding Transformers, the Data Science Way [[blog]](https://www.kdnuggets.com/2020/10/understanding-transformers-data-science-way.html)
3. The Illustrated Transformer [[blog]](https://jalammar.github.io/illustrated-transformer/)
4. The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) [[blog]](https://jalammar.github.io/illustrated-bert/)
5. The Annotated Transformer [[web]](http://nlp.seas.harvard.edu/2018/04/03/attention.html)

### Few-shot Learning:
- (2022) Flamingo [[paper]](https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model) [[code]](https://github.com/lucidrains/flamingo-pytorch)
- Language Models are Few-Shot Learners [[paper]](https://arxiv.org/pdf/2005.14165.pdf)

### AutoEncoders
1. Building Autoencoders in Keras [[blog]](https://blog.keras.io/building-autoencoders-in-keras.html)
2. AutoenCODE [[repo]](https://github.com/micheletufano/AutoenCODE)
3. Autoencoder - denoising autoencoder [[YT]](https://www.youtube.com/watch?v=t2NQ_c5BFOc)
4. Autoencoder - contractive autoencoder [[YT]](https://www.youtube.com/watch?v=79sYlJ8Cvlc)
5. Ali Ghodsi, Lec : Deep Learning, Variational Autoencoder [[YT]](https://www.youtube.com/watch?v=uaaqyVS9-rM)
6. Deep learning - variational bound [[YT]](https://www.youtube.com/watch?v=pStDscJh2Wo)
7. Variational_AutoEncoder [[YT]](https://www.youtube.com/playlist?list=PLdxQ7SoCLQANizknbIiHzL_hYjEaI-wUe)
8. vae_maths [[repo]](https://github.com/AndrewSpano/Disentangled-Variational-Autoencoder/blob/main/mathematical_analysis/vae_maths.pdf)
9. Practicum: Variational autoencoders [[YT]](https://www.youtube.com/watch?v=7Rb4s9wNOmc)
10. Generative Models - Variational Autoencoders [[book]](https://atcold.github.io/pytorch-Deep-Learning/en/week08/08-3/)
11. Introducing Variational Autoencoders [[blog]](https://blog.fastforwardlabs.com/2016/08/12/introducing-variational-autoencoders-in-prose-and-code.html)
12. NSynth: Neural Audio Synthesis [[blog]](https://magenta.tensorflow.org/nsynth)
13. An Introduction to Variational Autoencoders [[paper]](https://arxiv.org/pdf/1906.02691.pdf)
14. Understanding Variational Autoencoders (VAEs) [[blog]](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73)

### Adversarial NETS
1. GANs [[web]](https://developers.google.com/machine-learning/gan)
2. Generative adversarial networks [[YT]](https://www.youtube.com/watch?v=xYc11zyZ26M)
3. Understanding Generative Adversarial Networks (GANs) [[blog]](https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29)
4. GAN Lab [[demo]](https://poloclub.github.io/ganlab/)
5. Generative Adversarial Networks [[tutorial]](https://arxiv.org/pdf/1701.00160.pdf)
